{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On importe les bilbliothéques qu'on va utilisé sur ce Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librairies\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import stop_words\n",
    "import re, sys\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On importe la table question ---> tag et on la transforme sous une forme exploitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### on cmmence par joindre les tables\n",
    "df = pd.read_csv('quesiton_tag_100_25_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notaire         76\n",
       "transmission    73\n",
       "patrimoine      52\n",
       "droit           46\n",
       "fiscalité       45\n",
       "banque          38\n",
       "assurance       26\n",
       "agent           17\n",
       "rendement       16\n",
       "crowdfunding     1\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'message_y':'message'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Rajoute chaque tag dans notre texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tags = df['tag'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(val):\n",
    "    val = \" \".join(str(x) for x in val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i+10000000 for i in range(len(top_tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame()\n",
    "a['id'] = index\n",
    "a['message'] = top_tags\n",
    "a['tag'] = top_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On garde une phrase avec tous les tags correspond par ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('message')['tag'].unique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg['Tag'] = dfg['tag'].apply(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dealing with message\n",
    "def correct_text(x):\n",
    "    chkr = enchant.checker.SpellChecker(\"fr_FR\")\n",
    "    chkr.set_text(x)\n",
    "    for err in chkr:\n",
    "        try:\n",
    "            sug = err.suggest()[0]\n",
    "            err.replace(sug)\n",
    "        except : pass\n",
    "    c = chkr.get_text()#returns corrected text\n",
    "    return c\n",
    "def tokenization(x):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+') # Use regular expression to delete \\n\n",
    "    x = tokenizer.tokenize(x.lower())\n",
    "    return x\n",
    "# Define stopwords\n",
    "sw = stop_words.get_stop_words(language='fr')\n",
    "def del_stopwords(x):\n",
    "    x = [t.lower() for t in re.split(\" \", re.sub(r\"(\\W+|_|\\d+)\", \" \", \" \".join(x)))\n",
    "                 if t.lower() not in sw and len(t) > 1]\n",
    "    return x\n",
    "lem = spacy.load('fr_core_news_md')\n",
    "def lematization(x):\n",
    "    x = [tok.lemma_ for tok in lem(' '.join(x))]\n",
    "    return x\n",
    "def preprocessing(x):\n",
    "    return lematization(del_stopwords(tokenization(correct_text(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg['tokens_clean_lemma'] = dfg['message'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['héritage',\n",
       " 'soeur',\n",
       " 'mèr',\n",
       " 'rester',\n",
       " 'succession',\n",
       " 'pèr',\n",
       " 'notair',\n",
       " 'dire',\n",
       " 'soeur',\n",
       " 'mer',\n",
       " 'rester',\n",
       " 'normal',\n",
       " 'merci',\n",
       " 'pro',\n",
       " 'réponse']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg['tokens_clean_lemma'].loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# on termine le preprocessing des messages on cree une nouvelle table data sur laquelle on entraine notre modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfg.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on entraine notre vectorizer tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['tokens_clean_lemma'].apply(trans).values\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1)) # Creating tf-idf matrix using unigram\n",
    "matrix = vectorizer.fit_transform(corpus) \n",
    "matrix_ = matrix.todense()\n",
    "tf_idfmatrix = pd.DataFrame(data=matrix_,columns=vectorizer.get_feature_names(), index=data.index)\n",
    "tf_idfmatrix = pd.concat([data['message'],data['tokens_clean_lemma'],tf_idfmatrix, data['Tag']], ignore_index=False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on définie le score MAPK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=5):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=5):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On définit la Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X,y,test_size,k,n_splits=5):\n",
    "    cv = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0)\n",
    "    scores = []\n",
    "    for indice_train, indice_test in cv.split(X):\n",
    "        X_train = X[indice_train]\n",
    "        X_test = X[indice_test]\n",
    "        y_train = y[indice_train]\n",
    "        y_test = y[indice_test]\n",
    "        #     Target binarizing\n",
    "        mlb = MultiLabelBinarizer(classes=df['tag'].unique())\n",
    "        mlb.fit([w.split() for w in y_train])\n",
    "        y_train_mlb = mlb.transform([w.split() for w in y_train])\n",
    "        y_test_mlb = mlb.transform([w.split() for w in y_test])\n",
    "        m_NB = OneVsRestClassifier(MultinomialNB(alpha=0.001))\n",
    "        m_NB.fit(X_train[:,2:], y_train_mlb)\n",
    "        y_pred_NB = mlb.classes_[np.argsort(m_NB.predict_proba(X_test[:,2:]))][:, :-(200+1): -1]\n",
    "        c = mlb.inverse_transform(y_test_mlb)\n",
    "        y_ij = []\n",
    "        for elem in c:\n",
    "            a = []\n",
    "            for i in range(len(elem)):\n",
    "                a.append(elem[i])\n",
    "            y_ij.append(a)\n",
    "        scores.append(mapk(y_ij, y_pred_NB, k=k))\n",
    "    print(indice_train.shape,indice_test.shape)\n",
    "    print('scores :',scores)\n",
    "    print('mean',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On teste avec plusieurs paramétre k à chaque fois on change le test_size (20% , 30% , 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,) (30,)\n",
      "scores : [0.7, 0.6333333333333333, 0.8, 0.6666666666666666, 0.7]\n",
      "mean 0.7\n",
      "(120,) (30,)\n",
      "scores : [0.5583333333333333, 0.65, 0.6833333333333333, 0.5666666666666667, 0.6583333333333333]\n",
      "mean 0.6233333333333333\n",
      "(120,) (30,)\n",
      "scores : [0.6101851851851852, 0.6583333333333333, 0.6518518518518519, 0.5703703703703702, 0.6398148148148147]\n",
      "mean 0.6261111111111111\n",
      "(120,) (30,)\n",
      "scores : [0.639351851851852, 0.6641203703703703, 0.6643518518518517, 0.6226851851851851, 0.6814814814814815]\n",
      "mean 0.6543981481481481\n",
      "(120,) (30,)\n",
      "scores : [0.6427962962962963, 0.6690925925925927, 0.6853518518518519, 0.649351851851852, 0.6842870370370371]\n",
      "mean 0.6661759259259259\n",
      "(120,) (30,)\n",
      "scores : [0.6799629629629628, 0.6983518518518519, 0.7168333333333333, 0.673425925925926, 0.7035462962962965]\n",
      "mean 0.6944240740740741\n",
      "(120,) (30,)\n",
      "scores : [0.7037724867724867, 0.7142248677248677, 0.7444523809523811, 0.6861243386243387, 0.7264034391534392]\n",
      "mean 0.7149955026455027\n"
     ]
    }
   ],
   "source": [
    "X = tf_idfmatrix.iloc[:, :-1].values\n",
    "y = tf_idfmatrix['Tag']\n",
    "for k in range(1,8):\n",
    "    cross_val(X,y,0.2,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K  1\n",
      "(105,) (45,)\n",
      "scores : [0.6888888888888889, 0.6666666666666666, 0.7333333333333333, 0.7555555555555555, 0.6666666666666666]\n",
      "mean 0.7022222222222222\n",
      "K  2\n",
      "(105,) (45,)\n",
      "scores : [0.5888888888888889, 0.6555555555555556, 0.65, 0.6555555555555556, 0.5722222222222222]\n",
      "mean 0.6244444444444444\n",
      "K  3\n",
      "(105,) (45,)\n",
      "scores : [0.5962962962962962, 0.625925925925926, 0.6425925925925925, 0.6691358024691358, 0.5604938271604938]\n",
      "mean 0.6188888888888888\n",
      "K  4\n",
      "(105,) (45,)\n",
      "scores : [0.6453703703703704, 0.6277777777777778, 0.6594135802469135, 0.6748456790123457, 0.6087962962962962]\n",
      "mean 0.6432407407407408\n",
      "K  5\n",
      "(105,) (45,)\n",
      "scores : [0.6352222222222221, 0.6532592592592593, 0.6735246913580246, 0.7104012345679013, 0.6401851851851851]\n",
      "mean 0.6625185185185185\n",
      "K  6\n",
      "(105,) (45,)\n",
      "scores : [0.6698765432098764, 0.6730123456790124, 0.6992037037037035, 0.733858024691358, 0.6661111111111111]\n",
      "mean 0.6884123456790123\n",
      "K  7\n",
      "(105,) (45,)\n",
      "scores : [0.691463844797178, 0.693541446208113, 0.7292566137566138, 0.7391490299823632, 0.6817724867724867]\n",
      "mean 0.707036684303351\n"
     ]
    }
   ],
   "source": [
    "X = tf_idfmatrix.iloc[:, :-1].values\n",
    "y = tf_idfmatrix['Tag']\n",
    "for k in range(1,8):\n",
    "    print(\"K \" , k)\n",
    "    cross_val(X,y,0.3,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75,) (75,)\n",
      "scores : [0.7333333333333333, 0.72, 0.5866666666666667, 0.6666666666666666, 0.6666666666666666]\n",
      "mean 0.6746666666666666\n",
      "(75,) (75,)\n",
      "scores : [0.66, 0.7066666666666667, 0.5466666666666666, 0.61, 0.61]\n",
      "mean 0.6266666666666667\n",
      "(75,) (75,)\n",
      "scores : [0.6581481481481481, 0.7174074074074074, 0.5525925925925925, 0.6381481481481482, 0.6303703703703704]\n",
      "mean 0.6393333333333333\n",
      "(75,) (75,)\n",
      "scores : [0.6758333333333333, 0.7210185185185185, 0.5949074074074074, 0.6390740740740741, 0.6604629629629629]\n",
      "mean 0.6582592592592593\n",
      "(75,) (75,)\n",
      "scores : [0.7006777777777778, 0.7279074074074073, 0.6217518518518518, 0.6696851851851852, 0.6857851851851852]\n",
      "mean 0.6811614814814815\n",
      "(75,) (75,)\n",
      "scores : [0.7200481481481482, 0.7496851851851852, 0.653788888888889, 0.6891666666666666, 0.7004518518518519]\n",
      "mean 0.7026281481481481\n",
      "(75,) (75,)\n",
      "scores : [0.7342703703703705, 0.7630185185185184, 0.6680111111111111, 0.7028174603174603, 0.7074359788359788]\n",
      "mean 0.7151106878306879\n"
     ]
    }
   ],
   "source": [
    "X = tf_idfmatrix.iloc[:, :-1].values\n",
    "y = tf_idfmatrix['Tag']\n",
    "for k in range(1,8):\n",
    "    cross_val(X,y,0.5,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrainement du modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on entraine notre modéle sur la partie contenant les tags dans ses textes\n",
    "tr_tags = tf_idfmatrix[tf_idfmatrix['message'].isin(top_tags)]\n",
    "tf_idfmatrix1 = tf_idfmatrix[~(tf_idfmatrix['message'].isin(top_tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size :  42\n",
      "train size :  108\n"
     ]
    }
   ],
   "source": [
    "# Splitting data in training and test set\n",
    "X = tf_idfmatrix1.iloc[:, :-1]\n",
    "y = tf_idfmatrix1['Tag']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3)\n",
    "X_train = pd.concat([X_train,tr_tags.iloc[:, :-1]])\n",
    "y_train = pd.concat([y_train,tr_tags['Tag']])\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "#     Target binarizing\n",
    "mlb = MultiLabelBinarizer(classes=df['tag'].unique())\n",
    "mlb.fit([w.split() for w in y_train])\n",
    "y_train_mlb = mlb.transform([w.split() for w in y_train])\n",
    "y_test_mlb = mlb.transform([w.split() for w in y_test])\n",
    "print('test size : ', X_test.shape[0])\n",
    "print('train size : ', y_train_mlb.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 1980) (42, 1980)\n",
      "0.7619047619047619\n",
      "0.6785714285714286\n",
      "0.6402116402116401\n",
      "0.6755952380952381\n",
      "0.7077579365079365\n",
      "0.7302447089947091\n",
      "0.7420360922146637\n"
     ]
    }
   ],
   "source": [
    "m_NB = OneVsRestClassifier(MultinomialNB(alpha=0.001))\n",
    "m_NB.fit(X_train[:,2:], y_train_mlb)\n",
    "y_pred_NB = mlb.classes_[np.argsort(m_NB.predict_proba(X_test[:,2:]))][:, :-(200+1): -1]\n",
    "c = mlb.inverse_transform(y_test_mlb)\n",
    "y_ij = []\n",
    "print(X_train.shape,X_test.shape)\n",
    "for elem in c:\n",
    "    a = []\n",
    "    for i in range(len(elem)):\n",
    "        a.append(elem[i])\n",
    "    y_ij.append(a)\n",
    "for k in range(1,8):\n",
    "    print(mapk(y_ij, y_pred_NB, k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on sauvegarde les outils qu'on a entrainer\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"model_supervised_fr.pkl\",\"wb\")\n",
    "pickle.dump(m_NB, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "\n",
    "pickle_out = open(\"binarizer_supervised_fr.pkl\",\"wb\")\n",
    "pickle.dump(mlb, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "\n",
    "pickle_out = open(\"vectorizer_tfidf_fr.pkl\",\"wb\")\n",
    "pickle.dump(vectorizer, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on affiche la table avec les questions de la partie du test ainsi que les tags prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = m_NB.predict_proba(X_test[:,2:])\n",
    "scores.sort()\n",
    "scores = scores[:,:-200:-1]\n",
    "clas = y_pred_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_proba = []\n",
    "for i in range(clas.shape[0]):\n",
    "    doc = []\n",
    "    for j in range(clas.shape[1]):\n",
    "        doc.append((clas[i][j],scores[i][j]))\n",
    "    clas_proba.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(X_test[:,:2]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(clas_proba).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag = final.merge(pred,how='inner',on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag['true'] = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tager un nouveau texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"vectorizer_tfidf_fr.pkl\",\"rb\")\n",
    "vv = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"model_supervised_fr.pkl\",\"rb\")\n",
    "mm = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    d = pd.DataFrame({'message':[text]})\n",
    "    d['tokens_clean_lemma'] = d['message'].apply(preprocessing)\n",
    "    corpus1 = d['tokens_clean_lemma'].apply(trans).values\n",
    "    matrix = vv.transform(corpus1) \n",
    "    matrix_ = matrix.todense()\n",
    "    X = pd.DataFrame(data=matrix_,columns=vv.get_feature_names(), index=d.index)\n",
    "    y_pred_NB = mlb.classes_[np.argsort(mm.predict_proba(X))][:, :-(200+1): -1]\n",
    "    scores = mm.predict_proba(X)\n",
    "    scores.sort()\n",
    "    scores = scores[:,:-200:-1]\n",
    "    clas = y_pred_NB\n",
    "    clas_proba = []\n",
    "    for i in range(clas.shape[0]):\n",
    "        doc = []\n",
    "        for j in range(clas.shape[1]):\n",
    "            doc.append((clas[i][j],scores[i][j]))\n",
    "        clas_proba.append(doc)\n",
    "    return list(y_pred_NB[0][:5]),clas_proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ici on éfinit une méthode pour tager les questions\n",
    "def tager(clas_proba,threshold = 0.6,nbr_tag=5):\n",
    "    count = 0\n",
    "    tagf = []\n",
    "    score = 0\n",
    "    for tag in clas_proba:\n",
    "        if tag[1]>threshold:\n",
    "            tagf.append(tag)\n",
    "            count += 1\n",
    "        else :\n",
    "            while score<threshold:\n",
    "                score += tag[1]\n",
    "                tagf.append(tag)\n",
    "                break\n",
    "    if len(tagf)>nbr_tag:\n",
    "        return tagf[:nbr_tag]\n",
    "    elif count == 0:\n",
    "        return 'pas de TAG'\n",
    "    else :\n",
    "        return tagf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patrimoine', 0.8188278421001546),\n",
       " ('notaire', 0.6458554269156233),\n",
       " ('transmission', 0.5501330170872044)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tager(predict(\"je veux louer une maison\")[1],threshold=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
